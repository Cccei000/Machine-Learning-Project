{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import jieba\n",
    "import random\n",
    "import imblearn\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "filelist = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trancate_txt(srgname,trgname):\n",
    "    with open(srgname,'r',encoding = 'UTF-8-sig') as srg:\n",
    "        trg = open(trgname,'w',encoding = 'UTF-8-sig')\n",
    "        lines = srg.readlines()\n",
    "        length = len(lines)\n",
    "        tag = 0\n",
    "        for i in range(length):\n",
    "            line = lines[i].replace(' ','').replace('\\u3000','')\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            if '主诉' in line:\n",
    "                tag = 1\n",
    "            if tag == 0:\n",
    "                continue\n",
    "            if ('既往史' in line or '过敏史' in line or \n",
    "                '个人史' in line or '婚育史' in line or \n",
    "                '家族史' in line):\n",
    "                continue\n",
    "            if '体格检查' in line:\n",
    "                trg.write(line)\n",
    "                line1 = lines[i+1].replace(' ','').replace('\\u3000','')\n",
    "                line2 = lines[i+2].replace(' ','').replace('\\u3000','')\n",
    "                if len(line1)<100:\n",
    "                    trg.write(line1)\n",
    "                    trg.write(line2)\n",
    "                elif len(line1)>100:\n",
    "                    trg.write(line1)\n",
    "                break\n",
    "            else:\n",
    "                trg.write(line)\n",
    "        trg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filelist:\n",
    "    trancate_txt(path+filename,'data_trancated/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('data_trancated/'):\n",
    "    src = open('data_trancated/'+filename,'r',encoding = 'UTF-8-sig')\n",
    "    trg = open('symptoms/'+filename,'w',encoding = 'UTF-8-sig')\n",
    "    trg.write(src.readline())\n",
    "    trg.write(src.readline())\n",
    "    src.close()\n",
    "    trg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ⊙∩～~`!@#$%^&*()_-+={}[]|\\:;\\\"\\'<>＜＞≪≫,.?/~·•！@#￥%……&*（）\\\\\\\n",
    "——-—+=｛【】｝』『「」〖〗|、：；“”‘’《》，。？、/*-+.\\t\\r\\n'\n",
    "trans = str.maketrans(punctuation,' '*len(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenamelist = os.listdir('symptoms/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Icarus\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.704 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "for name in filenamelist:\n",
    "    file = open('symptoms/'+name,'r',encoding = 'UTF-8-sig')\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    content = content.translate(trans)\n",
    "    content = jieba.lcut(content)\n",
    "    data.append(' '.join(content).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "illnessname = ['肾病','酮症','心脏病','眼病','周围神经病','足病']\n",
    "for name in filenamelist:\n",
    "    illness = name.split('_')[0]\n",
    "    for i in range(len(illnessname)):\n",
    "        if illness == illnessname[i]:\n",
    "            labels.append(i+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt','r',encoding = 'UTF-8-sig') as file:\n",
    "    stopwords = file.read()\n",
    "    stopwords = stopwords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = data.copy()\n",
    "data = []\n",
    "for line in data_tmp:\n",
    "    line_tmp = line.copy()\n",
    "    for word in line:\n",
    "        if word in stopwords:\n",
    "            line_tmp.remove(word)\n",
    "    data.append(line_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Doc2Vec(documents, vector_size=256, window=3, min_count=10, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(datalabel):\n",
    "    for i in range(6):\n",
    "        print(\"Class {} in training data:\".format(i+1),(datalabel == i+1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocData = np.array([model.infer_vector(line) for line in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = np.array(random.sample(range(DocData.shape[0]),300))\n",
    "train_index = np.setdiff1d(np.arange(DocData.shape[0]),valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training data before sampling:\n",
      "Class 1 in training data: 373\n",
      "Class 2 in training data: 230\n",
      "Class 3 in training data: 8\n",
      "Class 4 in training data: 110\n",
      "Class 5 in training data: 254\n",
      "Class 6 in training data: 77\n",
      "In validation data:\n",
      "Class 1 in training data: 94\n",
      "Class 2 in training data: 69\n",
      "Class 3 in training data: 7\n",
      "Class 4 in training data: 32\n",
      "Class 5 in training data: 75\n",
      "Class 6 in training data: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"In training data before sampling:\")\n",
    "data_summary(labels[train_index])\n",
    "print('In validation data:')\n",
    "data_summary(labels[valid_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_label = imblearn.over_sampling.ADASYN(n_neighbors=5).fit_sample(DocData[train_index],labels[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training data after sampling:\n",
      "Class 1 in training data: 373\n",
      "Class 2 in training data: 366\n",
      "Class 3 in training data: 376\n",
      "Class 4 in training data: 371\n",
      "Class 5 in training data: 367\n",
      "Class 6 in training data: 399\n"
     ]
    }
   ],
   "source": [
    "print(\"In training data after sampling:\")\n",
    "data_summary(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=MLPClassifier(activation='relu', alpha=1e-05,\n",
       "                                           batch_size='auto', beta_1=0.9,\n",
       "                                           beta_2=0.999, early_stopping=False,\n",
       "                                           epsilon=1e-08,\n",
       "                                           hidden_layer_sizes=(100, 50, 25),\n",
       "                                           learning_rate='constant',\n",
       "                                           learning_rate_init=0.001,\n",
       "                                           max_iter=200, momentum=0.9,\n",
       "                                           n_iter_no_change=10,\n",
       "                                           nesterovs_momentum=True, power_t=0.5,\n",
       "                                           random_state=1, shuffle=True,\n",
       "                                           solver='lbfgs', tol=0.0001,\n",
       "                                           validation_fraction=0.1,\n",
       "                                           verbose=False, warm_start=False),\n",
       "                   n_jobs=None)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsOneClassifier(MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,50,25,), random_state=1))\n",
    "clf.fit(train_data,train_label)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1    0.90323   0.67560   0.77301       373\n",
      "           2    0.81728   0.90437   0.85863       366\n",
      "           3    0.99208   1.00000   0.99603       376\n",
      "           4    0.94366   0.72237   0.81832       371\n",
      "           5    0.69803   0.86921   0.77427       367\n",
      "           6    0.88393   0.99248   0.93506       399\n",
      "\n",
      "    accuracy                        0.86234      2252\n",
      "   macro avg    0.87304   0.86067   0.85922      2252\n",
      "weighted avg    0.87390   0.86234   0.86054      2252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = clf.predict(train_data)\n",
    "print(classification_report(train_label,results,digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1    0.60606   0.42553   0.50000        94\n",
      "           2    0.50602   0.60870   0.55263        69\n",
      "           3    0.50000   0.14286   0.22222         7\n",
      "           4    0.87500   0.65625   0.75000        32\n",
      "           5    0.44554   0.60000   0.51136        75\n",
      "           6    0.33333   0.34783   0.34043        23\n",
      "\n",
      "    accuracy                        0.52333       300\n",
      "   macro avg    0.54433   0.46353   0.47944       300\n",
      "weighted avg    0.54823   0.52333   0.52290       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = clf.predict(DocData[valid_index])\n",
    "print(classification_report(labels[valid_index],results,digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "pract_data,pract_label = imblearn.over_sampling.ADASYN(n_neighbors=5).fit_sample(DocData,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 in training data: 467\n",
      "Class 2 in training data: 478\n",
      "Class 3 in training data: 468\n",
      "Class 4 in training data: 468\n",
      "Class 5 in training data: 473\n",
      "Class 6 in training data: 472\n"
     ]
    }
   ],
   "source": [
    "data_summary(pract_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1    0.90000   0.65525   0.75836       467\n",
      "           2    0.72822   0.69900   0.71331       299\n",
      "           3    0.13115   0.53333   0.21053        15\n",
      "           4    0.80124   0.90845   0.85149       142\n",
      "           5    0.71067   0.76900   0.73869       329\n",
      "           6    0.64626   0.95000   0.76923       100\n",
      "\n",
      "    accuracy                        0.73964      1352\n",
      "   macro avg    0.65292   0.75250   0.67360      1352\n",
      "weighted avg    0.77827   0.73964   0.74812      1352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsOneClassifier(MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,50,25,),random_state=1))\n",
    "clf.fit(pract_data,pract_label)    \n",
    "results = clf.predict(DocData)\n",
    "print(classification_report(labels,results,digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"test data/\"):\n",
    "    trancate_txt(\"test data/\"+filename,'test_trancated/'+filename)\n",
    "for filename in os.listdir('test_trancated/'):\n",
    "    src = open('test_trancated/'+filename,'r',encoding = 'UTF-8-sig')\n",
    "    trg = open('test_symptoms/'+filename,'w',encoding = 'UTF-8-sig')\n",
    "    trg.write(src.readline())\n",
    "    trg.write(src.readline())\n",
    "    src.close()\n",
    "    trg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for name in os.listdir(\"test_symptoms/\"):\n",
    "    file = open('test_symptoms/'+name,'r',encoding = 'UTF-8-sig')\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    content = content.translate(trans)\n",
    "    content = jieba.lcut(content)\n",
    "    test_data.append(' '.join(content).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = test_data.copy()\n",
    "test_data = []\n",
    "for line in data_tmp:\n",
    "    line_tmp = line.copy()\n",
    "    for word in line:\n",
    "        if word in stopwords:\n",
    "            line_tmp.remove(word)\n",
    "    test_data.append(line_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([model.infer_vector(line) for line in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist = os.listdir(\"test_symptoms/\")\n",
    "for i in range(len(results)):\n",
    "    shutil.copyfile(\"test data/\"+namelist[i],str(results[i])+'/'+namelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
